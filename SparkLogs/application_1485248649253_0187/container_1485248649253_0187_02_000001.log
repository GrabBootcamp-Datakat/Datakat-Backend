log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /opt/hdfs/spark.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:142)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
	at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)
	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)
	at org.apache.spark.Logging$class.initializeLogging(Logging.scala:121)
	at org.apache.spark.Logging$class.initializeIfNecessary(Logging.scala:106)
	at org.apache.spark.Logging$class.log(Logging.scala:50)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.log(ApplicationMaster.scala:635)
	at org.apache.spark.deploy.yarn.ApplicationMaster$.main(ApplicationMaster.scala:649)
	at org.apache.spark.deploy.yarn.ExecutorLauncher$.main(ApplicationMaster.scala:674)
	at org.apache.spark.deploy.yarn.ExecutorLauncher.main(ApplicationMaster.scala)
log4j:ERROR Either File or DatePattern options are not set for appender [FILE].
17/07/27 21:38:33 INFO ApplicationMaster: Registered signal handlers for [TERM, HUP, INT]
17/07/27 21:38:34 INFO ApplicationMaster: ApplicationAttemptId: appattempt_1485248649253_0187_000002
17/07/27 21:38:35 INFO SecurityManager: Changing view acls to: yarn,curi
17/07/27 21:38:35 INFO SecurityManager: Changing modify acls to: yarn,curi
17/07/27 21:38:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(yarn, curi); users with modify permissions: Set(yarn, curi)
17/07/27 21:38:35 INFO ApplicationMaster: Waiting for Spark driver to be reachable.
17/07/27 21:38:35 INFO ApplicationMaster: Driver now available: 10.10.34.11:54324
17/07/27 21:38:35 INFO ApplicationMaster$AMEndpoint: Add WebUI Filter. AddWebUIFilter(org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter,Map(PROXY_HOSTS -> mesos-master-1, PROXY_URI_BASES -> http://mesos-master-1:8088/proxy/application_1485248649253_0187),/proxy/application_1485248649253_0187)
17/07/27 21:38:35 INFO RMProxy: Connecting to ResourceManager at mesos-master-1/10.10.34.11:8030
17/07/27 21:38:35 INFO YarnRMClient: Registering the ApplicationMaster
17/07/27 21:38:36 INFO YarnAllocator: Will request 16 executor containers, each with 5 cores and 28160 MB memory including 2560 MB overhead
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO YarnAllocator: Container request (host: Any, capability: <memory:28160, vCores:5>)
17/07/27 21:38:36 INFO ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-27:49350
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-22:42600
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-15:33449
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-09:51130
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000002 for on host mesos-slave-27
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-27
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000003 for on host mesos-slave-22
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-22
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000004 for on host mesos-slave-15
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-15
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000005 for on host mesos-slave-09
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-09
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO YarnAllocator: Received 4 containers from YARN, launching executors on 4 of them.
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-22:8042/node/containerlogs/container_1485248649253_0187_02_000003/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-22:8042/node/containerlogs/container_1485248649253_0187_02_000003/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 2 --hostname mesos-slave-22 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-27:8042/node/containerlogs/container_1485248649253_0187_02_000002/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-27:8042/node/containerlogs/container_1485248649253_0187_02_000002/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 1 --hostname mesos-slave-27 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-15:8042/node/containerlogs/container_1485248649253_0187_02_000004/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-15:8042/node/containerlogs/container_1485248649253_0187_02_000004/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 3 --hostname mesos-slave-15 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-09:8042/node/containerlogs/container_1485248649253_0187_02_000005/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-09:8042/node/containerlogs/container_1485248649253_0187_02_000005/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 4 --hostname mesos-slave-09 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-27:49350
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-09:51130
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-15:33449
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-22:42600
Exception in thread "ContainerLauncher-3" Exception in thread "ContainerLauncher-2" java.lang.Error: org.apache.spark.SparkException: Exception while starting container container_1485248649253_0187_02_000005 on host mesos-slave-09
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Exception while starting container container_1485248649253_0187_02_000005 on host mesos-slave-09
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:125)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	... 2 more
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Unauthorized request to start container. 
This token is expired. current time is 1501163118927 found 1501162978221
Note: System times on machines may be out of sync. Check system time and time zones.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateException(SerializedExceptionPBImpl.java:168)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.deSerialize(SerializedExceptionPBImpl.java:106)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:206)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:122)
	... 4 more
java.lang.Error: org.apache.spark.SparkException: Exception while starting container container_1485248649253_0187_02_000004 on host mesos-slave-15
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Exception while starting container container_1485248649253_0187_02_000004 on host mesos-slave-15
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:125)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	... 2 more
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Unauthorized request to start container. 
This token is expired. current time is 1501162988520 found 1501162978220
Note: System times on machines may be out of sync. Check system time and time zones.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateException(SerializedExceptionPBImpl.java:168)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.deSerialize(SerializedExceptionPBImpl.java:106)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:206)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:122)
	... 4 more
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-14:39461
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-16:43341
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-master-2:41860
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-13:38324
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-06:34105
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-10:60295
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-08:38529
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-master-3:42043
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-07:39967
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-19:35680
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-slave-26:36438
17/07/27 21:38:36 INFO AMRMClientImpl: Received new token for : mesos-master-1:35426
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000006 for on host mesos-slave-14
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-14
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000007 for on host mesos-slave-16
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-16
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000008 for on host mesos-master-2
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-master-2
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000009 for on host mesos-slave-13
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-13
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000010 for on host mesos-slave-06
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-06
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000011 for on host mesos-slave-10
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-10
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000012 for on host mesos-slave-08
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-08
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000013 for on host mesos-master-3
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-master-3
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000014 for on host mesos-slave-07
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-14:8042/node/containerlogs/container_1485248649253_0187_02_000006/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-14:8042/node/containerlogs/container_1485248649253_0187_02_000006/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 5 --hostname mesos-slave-14 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-07
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-14:39461
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000015 for on host mesos-slave-19
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-19
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000016 for on host mesos-slave-26
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-slave-26
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Launching container container_1485248649253_0187_02_000017 for on host mesos-master-1
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.10.34.11:54324,  executorHostname: mesos-master-1
17/07/27 21:38:36 INFO ExecutorRunnable: Starting Executor Container
17/07/27 21:38:36 INFO YarnAllocator: Received 12 containers from YARN, launching executors on 12 of them.
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/07/27 21:38:36 INFO ExecutorRunnable: Setting up ContainerLaunchContext
17/07/27 21:38:36 INFO ExecutorRunnable: Preparing Local resources
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: Prepared Local resources Map(__spark__.jar -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar" } size: 109525492 timestamp: 1501162373987 type: FILE visibility: PRIVATE, pyspark.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip" } size: 355358 timestamp: 1501162374061 type: FILE visibility: PRIVATE, py4j-0.9-src.zip -> resource { scheme: "hdfs" host: "10.10.34.11" port: 9000 file: "/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip" } size: 44846 timestamp: 1501162374082 type: FILE visibility: PRIVATE)
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-13:8042/node/containerlogs/container_1485248649253_0187_02_000009/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-13:8042/node/containerlogs/container_1485248649253_0187_02_000009/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 8 --hostname mesos-slave-13 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-13:38324
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-06:8042/node/containerlogs/container_1485248649253_0187_02_000010/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-06:8042/node/containerlogs/container_1485248649253_0187_02_000010/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 9 --hostname mesos-slave-06 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-06:34105
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-08:8042/node/containerlogs/container_1485248649253_0187_02_000012/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-08:8042/node/containerlogs/container_1485248649253_0187_02_000012/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 11 --hostname mesos-slave-08 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-10:8042/node/containerlogs/container_1485248649253_0187_02_000011/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-10:8042/node/containerlogs/container_1485248649253_0187_02_000011/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 10 --hostname mesos-slave-10 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-08:38529
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-master-2:8042/node/containerlogs/container_1485248649253_0187_02_000008/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-master-2:8042/node/containerlogs/container_1485248649253_0187_02_000008/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 7 --hostname mesos-master-2 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-10:60295
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-master-2:41860
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-16:8042/node/containerlogs/container_1485248649253_0187_02_000007/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-16:8042/node/containerlogs/container_1485248649253_0187_02_000007/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 6 --hostname mesos-slave-16 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-16:43341
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-07:8042/node/containerlogs/container_1485248649253_0187_02_000014/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-07:8042/node/containerlogs/container_1485248649253_0187_02_000014/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 13 --hostname mesos-slave-07 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-07:39967
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-26:8042/node/containerlogs/container_1485248649253_0187_02_000016/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-26:8042/node/containerlogs/container_1485248649253_0187_02_000016/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 15 --hostname mesos-slave-26 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-26:36438
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-master-1:8042/node/containerlogs/container_1485248649253_0187_02_000017/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-master-1:8042/node/containerlogs/container_1485248649253_0187_02_000017/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 16 --hostname mesos-master-1 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-master-1:35426
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-master-3:8042/node/containerlogs/container_1485248649253_0187_02_000013/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-master-3:8042/node/containerlogs/container_1485248649253_0187_02_000013/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 12 --hostname mesos-master-3 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-master-3:42043
17/07/27 21:38:36 INFO ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark__.jar<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_LOG_URL_STDERR -> http://mesos-slave-19:8042/node/containerlogs/container_1485248649253_0187_02_000015/curi/stderr?start=-4096
    SPARK_DIST_CLASSPATH -> /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> .sparkStaging/application_1485248649253_0187
    SPARK_YARN_CACHE_FILES_FILE_SIZES -> 109525492,355358,44846
    SPARK_USER -> curi
    SPARK_YARN_CACHE_FILES_VISIBILITIES -> PRIVATE,PRIVATE,PRIVATE
    SPARK_YARN_MODE -> true
    SPARK_YARN_CACHE_FILES_TIME_STAMPS -> 1501162373987,1501162374061,1501162374082
    PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.9-src.zip
    SPARK_LOG_URL_STDOUT -> http://mesos-slave-19:8042/node/containerlogs/container_1485248649253_0187_02_000015/curi/stdout?start=-4096
    SPARK_YARN_CACHE_FILES -> hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/spark-assembly-1.6.0-hadoop2.2.0.jar#__spark__.jar,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/pyspark.zip#pyspark.zip,hdfs://10.10.34.11:9000/user/curi/.sparkStaging/application_1485248649253_0187/py4j-0.9-src.zip#py4j-0.9-src.zip
  command:
    {{JAVA_HOME}}/bin/java -server -XX:OnOutOfMemoryError='kill %p' -Xms25600m -Xmx25600m -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=54324' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:MaxPermSize=256m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.10.34.11:54324 --executor-id 14 --hostname mesos-slave-19 --cores 5 --app-id application_1485248649253_0187 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
17/07/27 21:38:36 INFO ContainerManagementProtocolProxy: Opening proxy : mesos-slave-19:35680
Exception in thread "ContainerLauncher-7" java.lang.Error: org.apache.spark.SparkException: Exception while starting container container_1485248649253_0187_02_000007 on host mesos-slave-16
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Exception while starting container container_1485248649253_0187_02_000007 on host mesos-slave-16
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:125)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.run(ExecutorRunnable.scala:68)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	... 2 more
Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Unauthorized request to start container. 
This token is expired. current time is 1501163027310 found 1501162978675
Note: System times on machines may be out of sync. Check system time and time zones.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.instantiateException(SerializedExceptionPBImpl.java:168)
	at org.apache.hadoop.yarn.api.records.impl.pb.SerializedExceptionPBImpl.deSerialize(SerializedExceptionPBImpl.java:106)
	at org.apache.hadoop.yarn.client.api.impl.NMClientImpl.startContainer(NMClientImpl.java:206)
	at org.apache.spark.deploy.yarn.ExecutorRunnable.startContainer(ExecutorRunnable.scala:122)
	... 4 more
17/07/27 21:38:39 INFO AMRMClientImpl: Received new token for : mesos-slave-05:33209
17/07/27 21:38:39 INFO AMRMClientImpl: Received new token for : mesos-slave-21:57813
17/07/27 21:38:39 INFO AMRMClientImpl: Received new token for : mesos-slave-20:49854
17/07/27 21:38:39 INFO AMRMClientImpl: Received new token for : mesos-slave-28:39598
17/07/27 21:38:39 INFO AMRMClientImpl: Received new token for : mesos-slave-23:39541
17/07/27 21:38:39 INFO AMRMClientImpl: Received new token for : mesos-slave-17:46510
17/07/27 21:38:39 INFO YarnAllocator: Received 12 containers from YARN, launching executors on 0 of them.
17/07/27 21:40:49 INFO ApplicationMaster$AMEndpoint: Driver terminated or disconnected! Shutting down. mesos-master-1:54324
17/07/27 21:40:49 INFO ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
17/07/27 21:40:49 INFO ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
17/07/27 21:40:49 INFO AMRMClientImpl: Waiting for application to be successfully unregistered.
17/07/27 21:40:49 INFO ApplicationMaster: Deleting staging directory .sparkStaging/application_1485248649253_0187
17/07/27 21:40:49 INFO ShutdownHookManager: Shutdown hook called
